{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>quality_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.99380</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.033</td>\n",
       "      <td>11.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.99080</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.56</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>17.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.99470</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.040</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.99200</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>48.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.52</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.3</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.62</td>\n",
       "      <td>19.25</td>\n",
       "      <td>0.040</td>\n",
       "      <td>41.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.00020</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.67</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.032</td>\n",
       "      <td>28.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99140</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.55</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.98920</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.033</td>\n",
       "      <td>17.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.99170</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.14</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>34.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.98920</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>19.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.049</td>\n",
       "      <td>41.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.074</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99370</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.052</td>\n",
       "      <td>16.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.32</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.046</td>\n",
       "      <td>56.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.052</td>\n",
       "      <td>35.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.39</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.051</td>\n",
       "      <td>32.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.99610</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.047</td>\n",
       "      <td>17.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99140</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.49</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.033</td>\n",
       "      <td>37.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.99060</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.71</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.31</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.046</td>\n",
       "      <td>42.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.99324</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4869</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.33</td>\n",
       "      <td>10.10</td>\n",
       "      <td>0.032</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99626</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.28</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.021</td>\n",
       "      <td>29.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99188</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.36</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98970</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.55</td>\n",
       "      <td>12.050000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4872</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.41</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.032</td>\n",
       "      <td>50.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.030</td>\n",
       "      <td>33.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.99044</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4874</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.99282</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.035</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99245</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.41</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4876</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.038</td>\n",
       "      <td>34.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.99132</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.59</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4877</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.032</td>\n",
       "      <td>12.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.99286</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4878</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.035</td>\n",
       "      <td>6.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99234</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.35</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4879</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.533333</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.533333</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.27</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4882</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.037</td>\n",
       "      <td>45.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.99184</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.035</td>\n",
       "      <td>60.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.98964</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.38</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.048</td>\n",
       "      <td>68.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.99492</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.40</td>\n",
       "      <td>8.10</td>\n",
       "      <td>0.046</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.550000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.028</td>\n",
       "      <td>45.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99168</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.08</td>\n",
       "      <td>12.150000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.98928</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.79</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.052</td>\n",
       "      <td>38.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.99330</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.27</td>\n",
       "      <td>11.75</td>\n",
       "      <td>0.030</td>\n",
       "      <td>34.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.036</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.98938</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.038</td>\n",
       "      <td>38.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.99074</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0             0.270         0.36           20.70      0.045   \n",
       "1               6.3             0.300         0.34            1.60      0.049   \n",
       "2               8.1             0.280         0.40            6.90      0.050   \n",
       "3               7.2             0.230         0.32            8.50      0.058   \n",
       "4               7.2             0.230         0.32            8.50      0.058   \n",
       "5               8.1             0.280         0.40            6.90      0.050   \n",
       "6               6.2             0.320         0.16            7.00      0.045   \n",
       "7               7.0             0.270         0.36           20.70      0.045   \n",
       "8               6.3             0.300         0.34            1.60      0.049   \n",
       "9               8.1             0.220         0.43            1.50      0.044   \n",
       "10              8.1             0.270         0.41            1.45      0.033   \n",
       "11              8.6             0.230         0.40            4.20      0.035   \n",
       "12              7.9             0.180         0.37            1.20      0.040   \n",
       "13              6.6             0.160         0.40            1.50      0.044   \n",
       "14              8.3             0.420         0.62           19.25      0.040   \n",
       "15              6.6             0.170         0.38            1.50      0.032   \n",
       "16              6.3             0.480         0.04            1.10      0.046   \n",
       "17              6.2             0.660         0.48            1.20      0.029   \n",
       "18              7.4             0.340         0.42            1.10      0.033   \n",
       "19              6.5             0.310         0.14            7.50      0.044   \n",
       "20              6.2             0.660         0.48            1.20      0.029   \n",
       "21              6.4             0.310         0.38            2.90      0.038   \n",
       "22              6.8             0.260         0.42            1.70      0.049   \n",
       "23              7.6             0.670         0.14            1.50      0.074   \n",
       "24              6.6             0.270         0.41            1.30      0.052   \n",
       "25              7.0             0.250         0.32            9.00      0.046   \n",
       "26              6.9             0.240         0.35            1.00      0.052   \n",
       "27              7.0             0.280         0.39            8.70      0.051   \n",
       "28              7.4             0.270         0.48            1.10      0.047   \n",
       "29              7.2             0.320         0.36            2.00      0.033   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4868            5.8             0.230         0.31            4.50      0.046   \n",
       "4869            6.6             0.240         0.33           10.10      0.032   \n",
       "4870            6.1             0.320         0.28            6.60      0.021   \n",
       "4871            5.0             0.200         0.40            1.90      0.015   \n",
       "4872            6.0             0.420         0.41           12.40      0.032   \n",
       "4873            5.7             0.210         0.32            1.60      0.030   \n",
       "4874            5.6             0.200         0.36            2.50      0.048   \n",
       "4875            7.4             0.220         0.26            1.20      0.035   \n",
       "4876            6.2             0.380         0.42            2.50      0.038   \n",
       "4877            5.9             0.540         0.00            0.80      0.032   \n",
       "4878            6.2             0.530         0.02            0.90      0.035   \n",
       "4879            6.6             0.340         0.40            8.10      0.046   \n",
       "4880            6.6             0.340         0.40            8.10      0.046   \n",
       "4881            5.0             0.235         0.27           11.75      0.030   \n",
       "4882            5.5             0.320         0.13            1.30      0.037   \n",
       "4883            4.9             0.470         0.17            1.90      0.035   \n",
       "4884            6.5             0.330         0.38            8.30      0.048   \n",
       "4885            6.6             0.340         0.40            8.10      0.046   \n",
       "4886            6.2             0.210         0.28            5.70      0.028   \n",
       "4887            6.2             0.410         0.22            1.90      0.023   \n",
       "4888            6.8             0.220         0.36            1.20      0.052   \n",
       "4889            4.9             0.235         0.27           11.75      0.030   \n",
       "4890            6.1             0.340         0.29            2.20      0.036   \n",
       "4891            5.7             0.210         0.32            0.90      0.038   \n",
       "4892            6.5             0.230         0.38            1.30      0.032   \n",
       "4893            6.2             0.210         0.29            1.60      0.039   \n",
       "4894            6.6             0.320         0.36            8.00      0.047   \n",
       "4895            6.5             0.240         0.19            1.20      0.041   \n",
       "4896            5.5             0.290         0.30            1.10      0.022   \n",
       "4897            6.0             0.210         0.38            0.80      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "5                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "6                    30.0                 136.0  0.99490  3.18       0.47   \n",
       "7                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "8                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "9                    28.0                 129.0  0.99380  3.22       0.45   \n",
       "10                   11.0                  63.0  0.99080  2.99       0.56   \n",
       "11                   17.0                 109.0  0.99470  3.14       0.53   \n",
       "12                   16.0                  75.0  0.99200  3.18       0.63   \n",
       "13                   48.0                 143.0  0.99120  3.54       0.52   \n",
       "14                   41.0                 172.0  1.00020  2.98       0.67   \n",
       "15                   28.0                 112.0  0.99140  3.25       0.55   \n",
       "16                   30.0                  99.0  0.99280  3.24       0.36   \n",
       "17                   29.0                  75.0  0.98920  3.33       0.39   \n",
       "18                   17.0                 171.0  0.99170  3.12       0.53   \n",
       "19                   34.0                 133.0  0.99550  3.22       0.50   \n",
       "20                   29.0                  75.0  0.98920  3.33       0.39   \n",
       "21                   19.0                 102.0  0.99120  3.17       0.35   \n",
       "22                   41.0                 122.0  0.99300  3.47       0.48   \n",
       "23                   25.0                 168.0  0.99370  3.05       0.51   \n",
       "24                   16.0                 142.0  0.99510  3.42       0.47   \n",
       "25                   56.0                 245.0  0.99550  3.25       0.50   \n",
       "26                   35.0                 146.0  0.99300  3.45       0.44   \n",
       "27                   32.0                 141.0  0.99610  3.38       0.53   \n",
       "28                   17.0                 132.0  0.99140  3.19       0.49   \n",
       "29                   37.0                 114.0  0.99060  3.10       0.71   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4868                 42.0                 124.0  0.99324  3.31       0.64   \n",
       "4869                  8.0                  81.0  0.99626  3.19       0.51   \n",
       "4870                 29.0                 132.0  0.99188  3.15       0.36   \n",
       "4871                 20.0                  98.0  0.98970  3.37       0.55   \n",
       "4872                 50.0                 179.0  0.99622  3.14       0.60   \n",
       "4873                 33.0                 122.0  0.99044  3.33       0.52   \n",
       "4874                 16.0                 125.0  0.99282  3.49       0.49   \n",
       "4875                 18.0                  97.0  0.99245  3.12       0.41   \n",
       "4876                 34.0                 117.0  0.99132  3.36       0.59   \n",
       "4877                 12.0                  82.0  0.99286  3.25       0.36   \n",
       "4878                  6.0                  81.0  0.99234  3.24       0.35   \n",
       "4879                 68.0                 170.0  0.99494  3.15       0.50   \n",
       "4880                 68.0                 170.0  0.99494  3.15       0.50   \n",
       "4881                 34.0                 118.0  0.99540  3.07       0.50   \n",
       "4882                 45.0                 156.0  0.99184  3.26       0.38   \n",
       "4883                 60.0                 148.0  0.98964  3.27       0.35   \n",
       "4884                 68.0                 174.0  0.99492  3.14       0.50   \n",
       "4885                 68.0                 170.0  0.99494  3.15       0.50   \n",
       "4886                 45.0                 121.0  0.99168  3.21       1.08   \n",
       "4887                  5.0                  56.0  0.98928  3.04       0.79   \n",
       "4888                 38.0                 127.0  0.99330  3.04       0.54   \n",
       "4889                 34.0                 118.0  0.99540  3.07       0.50   \n",
       "4890                 25.0                 100.0  0.98938  3.06       0.44   \n",
       "4891                 38.0                 121.0  0.99074  3.24       0.46   \n",
       "4892                 29.0                 112.0  0.99298  3.29       0.54   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "        alcohol  quality quality_labels  \n",
       "0      8.800000        6              1  \n",
       "1      9.500000        6              1  \n",
       "2     10.100000        6              1  \n",
       "3      9.900000        6              1  \n",
       "4      9.900000        6              1  \n",
       "5     10.100000        6              1  \n",
       "6      9.600000        6              1  \n",
       "7      8.800000        6              1  \n",
       "8      9.500000        6              1  \n",
       "9     11.000000        6              1  \n",
       "10    12.000000        5              0  \n",
       "11     9.700000        5              0  \n",
       "12    10.800000        5              0  \n",
       "13    12.400000        7              2  \n",
       "14     9.700000        5              0  \n",
       "15    11.400000        7              2  \n",
       "16     9.600000        6              1  \n",
       "17    12.800000        8              2  \n",
       "18    11.300000        6              1  \n",
       "19     9.500000        5              0  \n",
       "20    12.800000        8              2  \n",
       "21    11.000000        7              2  \n",
       "22    10.500000        8              2  \n",
       "23     9.300000        5              0  \n",
       "24    10.000000        6              1  \n",
       "25    10.400000        6              1  \n",
       "26    10.000000        6              1  \n",
       "27    10.500000        6              1  \n",
       "28    11.600000        6              1  \n",
       "29    12.300000        7              2  \n",
       "...         ...      ...            ...  \n",
       "4868  10.800000        6              1  \n",
       "4869   9.800000        6              1  \n",
       "4870  11.450000        7              2  \n",
       "4871  12.050000        6              1  \n",
       "4872   9.700000        5              0  \n",
       "4873  11.900000        6              1  \n",
       "4874  10.000000        6              1  \n",
       "4875   9.700000        6              1  \n",
       "4876  11.600000        7              2  \n",
       "4877   8.800000        5              0  \n",
       "4878   9.500000        4              0  \n",
       "4879   9.533333        6              1  \n",
       "4880   9.533333        6              1  \n",
       "4881   9.400000        6              1  \n",
       "4882  10.700000        5              0  \n",
       "4883  11.500000        6              1  \n",
       "4884   9.600000        5              0  \n",
       "4885   9.550000        6              1  \n",
       "4886  12.150000        7              2  \n",
       "4887  13.000000        7              2  \n",
       "4888   9.200000        5              0  \n",
       "4889   9.400000        6              1  \n",
       "4890  11.800000        6              1  \n",
       "4891  10.600000        6              1  \n",
       "4892   9.700000        5              0  \n",
       "4893  11.200000        6              1  \n",
       "4894   9.600000        5              0  \n",
       "4895   9.400000        6              1  \n",
       "4896  12.800000        7              2  \n",
       "4897  11.800000        6              1  \n",
       "\n",
       "[4898 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/winequality-white.csv', ';')\n",
    "\n",
    "#define splits: \n",
    "#3-5: bad; 6: medium; 7-9: good\n",
    "bins = [3,5,6,9]\n",
    "quality_labels=[0,1,2]\n",
    "\n",
    "data['quality_labels'] = pd.cut(data['quality'], bins=bins, labels=quality_labels, include_lowest=True)\n",
    "features_raw = data.drop(['quality', 'quality_labels'], axis=1)\n",
    "labels_raw = data['quality_labels']\n",
    "\n",
    "#utility function to display results\n",
    "def print_res(search_obj):\n",
    "    res = pd.DataFrame.from_dict(search_obj.cv_results_).sort_values('rank_test_score')\n",
    "    cols = [c for c in res.columns if c[:5] != 'split' and c[-4:] != 'time']\n",
    "    display(res[cols])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['quality'], bins=np.linspace(2.75,9.25,14), histtype='bar')\n",
    "plt.xlabel('quality label')\n",
    "plt.ylabel('number of wines');\n",
    "plt.savefig('images/unbalanced.eps', format='eps', dpi=1000)\n",
    "\n",
    "a = np.array(data['quality'])\n",
    "a[(a>7) | (a<5)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347151403973234"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(activation='logistic', learning_rate_init=.0005)\n",
    "scores = cross_val_score(model, features_raw, labels_raw, cv=5, scoring=None)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=1,\n",
       "          param_distributions={'activation': ['tanh', 'logistic', 'relu', 'identity'], 'alpha': [0.0001, 0.001, 0.01, 0.1, 1], 'hidden_layer_sizes': [(5,), (10,), (20,), (50,), (100,), (200,)], 'learning_rate_init': [0.1, 0.5, 0.01, 0.005, 0.001, 0.0005, 0.0001]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "model = MLPClassifier()\n",
    "\n",
    "distr = {'activation': ['tanh', 'logistic', 'relu', 'identity'],\n",
    "         'alpha': [1e-4, 1e-3, 1e-2, .1, 1],\n",
    "         'hidden_layer_sizes': [(5,),(10,),(20,),(50,),(100,),(200,)],\n",
    "         'learning_rate_init': [.1,.5,.01,.005,.001,.0005,.0001]}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=distr, n_iter=100, cv=5, return_train_score=True)\n",
    "random_search.fit(features_raw, labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate_init</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.535116</td>\n",
       "      <td>0.040196</td>\n",
       "      <td>1</td>\n",
       "      <td>0.557676</td>\n",
       "      <td>0.014917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.534096</td>\n",
       "      <td>0.050580</td>\n",
       "      <td>2</td>\n",
       "      <td>0.549051</td>\n",
       "      <td>0.011720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.534096</td>\n",
       "      <td>0.048170</td>\n",
       "      <td>2</td>\n",
       "      <td>0.528481</td>\n",
       "      <td>0.013078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.526337</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>4</td>\n",
       "      <td>0.530779</td>\n",
       "      <td>0.012896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.523071</td>\n",
       "      <td>0.028686</td>\n",
       "      <td>5</td>\n",
       "      <td>0.546806</td>\n",
       "      <td>0.018428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.517558</td>\n",
       "      <td>0.036487</td>\n",
       "      <td>6</td>\n",
       "      <td>0.544151</td>\n",
       "      <td>0.007283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.517558</td>\n",
       "      <td>0.058905</td>\n",
       "      <td>6</td>\n",
       "      <td>0.546601</td>\n",
       "      <td>0.004793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.025092</td>\n",
       "      <td>8</td>\n",
       "      <td>0.554870</td>\n",
       "      <td>0.015248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.512658</td>\n",
       "      <td>0.049466</td>\n",
       "      <td>9</td>\n",
       "      <td>0.534964</td>\n",
       "      <td>0.009001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.512250</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>10</td>\n",
       "      <td>0.521640</td>\n",
       "      <td>0.021714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.512250</td>\n",
       "      <td>0.045925</td>\n",
       "      <td>10</td>\n",
       "      <td>0.536903</td>\n",
       "      <td>0.010180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.510004</td>\n",
       "      <td>0.036210</td>\n",
       "      <td>12</td>\n",
       "      <td>0.541650</td>\n",
       "      <td>0.011985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.508779</td>\n",
       "      <td>0.048860</td>\n",
       "      <td>13</td>\n",
       "      <td>0.533281</td>\n",
       "      <td>0.021914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.507758</td>\n",
       "      <td>0.023074</td>\n",
       "      <td>14</td>\n",
       "      <td>0.556402</td>\n",
       "      <td>0.010285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.504696</td>\n",
       "      <td>0.033944</td>\n",
       "      <td>15</td>\n",
       "      <td>0.540221</td>\n",
       "      <td>0.008791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.502246</td>\n",
       "      <td>0.075831</td>\n",
       "      <td>16</td>\n",
       "      <td>0.491220</td>\n",
       "      <td>0.025069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.501633</td>\n",
       "      <td>0.016316</td>\n",
       "      <td>17</td>\n",
       "      <td>0.507554</td>\n",
       "      <td>0.029193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.501021</td>\n",
       "      <td>0.048461</td>\n",
       "      <td>18</td>\n",
       "      <td>0.522101</td>\n",
       "      <td>0.009915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.500204</td>\n",
       "      <td>0.053573</td>\n",
       "      <td>19</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>0.010981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.497346</td>\n",
       "      <td>0.051117</td>\n",
       "      <td>20</td>\n",
       "      <td>0.528229</td>\n",
       "      <td>0.024345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.495713</td>\n",
       "      <td>0.023894</td>\n",
       "      <td>21</td>\n",
       "      <td>0.510158</td>\n",
       "      <td>0.010530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.492650</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>22</td>\n",
       "      <td>0.510004</td>\n",
       "      <td>0.012051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.492446</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>23</td>\n",
       "      <td>0.508729</td>\n",
       "      <td>0.006890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.489792</td>\n",
       "      <td>0.042795</td>\n",
       "      <td>24</td>\n",
       "      <td>0.497961</td>\n",
       "      <td>0.027033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.485096</td>\n",
       "      <td>0.047097</td>\n",
       "      <td>25</td>\n",
       "      <td>0.514752</td>\n",
       "      <td>0.021719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.482646</td>\n",
       "      <td>0.027736</td>\n",
       "      <td>26</td>\n",
       "      <td>0.496019</td>\n",
       "      <td>0.026486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>27</td>\n",
       "      <td>0.482186</td>\n",
       "      <td>0.018857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.479992</td>\n",
       "      <td>0.056946</td>\n",
       "      <td>28</td>\n",
       "      <td>0.497194</td>\n",
       "      <td>0.014109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.479379</td>\n",
       "      <td>0.052097</td>\n",
       "      <td>29</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.012971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.478767</td>\n",
       "      <td>0.033793</td>\n",
       "      <td>30</td>\n",
       "      <td>0.495102</td>\n",
       "      <td>0.018910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>69</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>69</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>69</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>69</td>\n",
       "      <td>0.448755</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.447734</td>\n",
       "      <td>0.037776</td>\n",
       "      <td>75</td>\n",
       "      <td>0.449261</td>\n",
       "      <td>0.037354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.445284</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>76</td>\n",
       "      <td>0.458709</td>\n",
       "      <td>0.019880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.444875</td>\n",
       "      <td>0.034879</td>\n",
       "      <td>77</td>\n",
       "      <td>0.449981</td>\n",
       "      <td>0.038344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.444671</td>\n",
       "      <td>0.017378</td>\n",
       "      <td>78</td>\n",
       "      <td>0.462282</td>\n",
       "      <td>0.039597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.442425</td>\n",
       "      <td>0.022256</td>\n",
       "      <td>79</td>\n",
       "      <td>0.473663</td>\n",
       "      <td>0.011634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>80</td>\n",
       "      <td>0.433596</td>\n",
       "      <td>0.046929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.440996</td>\n",
       "      <td>0.037203</td>\n",
       "      <td>81</td>\n",
       "      <td>0.469834</td>\n",
       "      <td>0.009371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.438955</td>\n",
       "      <td>0.022309</td>\n",
       "      <td>82</td>\n",
       "      <td>0.472744</td>\n",
       "      <td>0.030580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.435688</td>\n",
       "      <td>0.056668</td>\n",
       "      <td>83</td>\n",
       "      <td>0.461367</td>\n",
       "      <td>0.076617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.428134</td>\n",
       "      <td>0.029407</td>\n",
       "      <td>84</td>\n",
       "      <td>0.459881</td>\n",
       "      <td>0.034246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.426092</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>85</td>\n",
       "      <td>0.426045</td>\n",
       "      <td>0.045633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.426092</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>85</td>\n",
       "      <td>0.425943</td>\n",
       "      <td>0.045582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.425888</td>\n",
       "      <td>0.045609</td>\n",
       "      <td>87</td>\n",
       "      <td>0.425988</td>\n",
       "      <td>0.045562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.425888</td>\n",
       "      <td>0.045609</td>\n",
       "      <td>87</td>\n",
       "      <td>0.425988</td>\n",
       "      <td>0.045562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.425888</td>\n",
       "      <td>0.045609</td>\n",
       "      <td>87</td>\n",
       "      <td>0.425988</td>\n",
       "      <td>0.045562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.403430</td>\n",
       "      <td>0.055815</td>\n",
       "      <td>90</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.055892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.055812</td>\n",
       "      <td>91</td>\n",
       "      <td>0.403176</td>\n",
       "      <td>0.055811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.055812</td>\n",
       "      <td>91</td>\n",
       "      <td>0.403278</td>\n",
       "      <td>0.055894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.402205</td>\n",
       "      <td>0.092963</td>\n",
       "      <td>93</td>\n",
       "      <td>0.402302</td>\n",
       "      <td>0.092933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.380359</td>\n",
       "      <td>0.055794</td>\n",
       "      <td>94</td>\n",
       "      <td>0.380409</td>\n",
       "      <td>0.055815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.379543</td>\n",
       "      <td>0.043471</td>\n",
       "      <td>95</td>\n",
       "      <td>0.388931</td>\n",
       "      <td>0.037822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.379339</td>\n",
       "      <td>0.092701</td>\n",
       "      <td>96</td>\n",
       "      <td>0.379536</td>\n",
       "      <td>0.092721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.113829</td>\n",
       "      <td>97</td>\n",
       "      <td>0.355811</td>\n",
       "      <td>0.113821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>identity</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.354635</td>\n",
       "      <td>0.078470</td>\n",
       "      <td>98</td>\n",
       "      <td>0.381637</td>\n",
       "      <td>0.055107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(5,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>relu</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.333197</td>\n",
       "      <td>0.037567</td>\n",
       "      <td>99</td>\n",
       "      <td>0.365348</td>\n",
       "      <td>0.042864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.287464</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>100</td>\n",
       "      <td>0.287465</td>\n",
       "      <td>0.058017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_learning_rate_init param_hidden_layer_sizes param_alpha  \\\n",
       "5                     0.005                   (100,)       0.001   \n",
       "89                   0.0005                   (100,)      0.0001   \n",
       "15                    0.005                   (200,)         0.1   \n",
       "86                    0.005                   (200,)        0.01   \n",
       "14                    0.005                    (20,)        0.01   \n",
       "75                   0.0001                   (100,)        0.01   \n",
       "66                    0.005                    (10,)      0.0001   \n",
       "65                   0.0005                    (50,)       0.001   \n",
       "45                    0.005                    (20,)         0.1   \n",
       "38                    0.005                    (50,)        0.01   \n",
       "24                   0.0005                    (20,)           1   \n",
       "61                    0.001                    (10,)       0.001   \n",
       "78                    0.005                   (100,)      0.0001   \n",
       "40                    0.001                    (50,)       0.001   \n",
       "67                   0.0001                   (100,)           1   \n",
       "0                       0.1                    (10,)        0.01   \n",
       "97                    0.005                    (10,)         0.1   \n",
       "48                     0.01                     (5,)         0.1   \n",
       "90                     0.01                    (20,)        0.01   \n",
       "39                    0.005                    (50,)      0.0001   \n",
       "50                   0.0001                   (200,)        0.01   \n",
       "19                   0.0005                    (50,)           1   \n",
       "30                     0.01                     (5,)           1   \n",
       "74                     0.01                   (200,)           1   \n",
       "31                     0.01                    (50,)        0.01   \n",
       "21                   0.0005                    (10,)           1   \n",
       "94                      0.5                    (10,)         0.1   \n",
       "82                      0.1                    (50,)        0.01   \n",
       "77                    0.001                    (50,)      0.0001   \n",
       "32                    0.001                    (50,)       0.001   \n",
       "..                      ...                      ...         ...   \n",
       "57                      0.5                     (5,)           1   \n",
       "27                      0.5                    (50,)         0.1   \n",
       "6                       0.5                    (50,)       0.001   \n",
       "22                      0.1                    (20,)      0.0001   \n",
       "25                     0.01                    (50,)      0.0001   \n",
       "64                      0.1                     (5,)           1   \n",
       "96                    0.005                    (50,)      0.0001   \n",
       "93                     0.01                   (100,)       0.001   \n",
       "8                     0.001                    (50,)        0.01   \n",
       "51                     0.01                   (200,)        0.01   \n",
       "69                    0.001                    (20,)        0.01   \n",
       "26                   0.0005                    (50,)       0.001   \n",
       "28                      0.5                   (100,)      0.0001   \n",
       "76                   0.0001                    (10,)         0.1   \n",
       "1                       0.5                   (200,)      0.0001   \n",
       "10                      0.5                   (100,)         0.1   \n",
       "73                      0.5                    (20,)        0.01   \n",
       "4                       0.5                    (50,)      0.0001   \n",
       "2                       0.5                    (20,)       0.001   \n",
       "84                      0.1                   (100,)         0.1   \n",
       "55                      0.5                    (10,)           1   \n",
       "53                      0.5                   (100,)      0.0001   \n",
       "71                      0.5                   (100,)         0.1   \n",
       "54                      0.1                    (20,)        0.01   \n",
       "91                   0.0001                     (5,)         0.1   \n",
       "13                      0.5                   (200,)           1   \n",
       "56                      0.5                    (50,)        0.01   \n",
       "47                    0.005                   (200,)       0.001   \n",
       "29                   0.0001                     (5,)         0.1   \n",
       "80                      0.5                    (20,)      0.0001   \n",
       "\n",
       "   param_activation                                             params  \\\n",
       "5          logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "89         logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "15         logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "86             tanh  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "14         logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "75             tanh  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "66         logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "65         logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "45         logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "38             relu  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "24             tanh  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "61         logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "78         logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "40             tanh  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "67             tanh  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "0          identity  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "97             tanh  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "48         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "90         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "39             tanh  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "50             relu  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "19             relu  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "30         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "74         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "31             tanh  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "21             relu  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "94         identity  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "82         identity  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "77         identity  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "32         identity  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "..              ...                                                ...   \n",
       "57             relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "27             relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "6              relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "22             relu  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "25         identity  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "64             relu  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "96         identity  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "93         identity  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "8          identity  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "51             relu  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "69         identity  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "26         identity  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "28         identity  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "76             relu  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "1          logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "10             relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "73             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "4              relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "2              relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "84             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "55             relu  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "53         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "71         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "54         logistic  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "91         identity  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "13             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "56             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "47         identity  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "29             relu  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "80             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  mean_train_score  \\\n",
       "5          0.535116        0.040196                1          0.557676   \n",
       "89         0.534096        0.050580                2          0.549051   \n",
       "15         0.534096        0.048170                2          0.528481   \n",
       "86         0.526337        0.043982                4          0.530779   \n",
       "14         0.523071        0.028686                5          0.546806   \n",
       "75         0.517558        0.036487                6          0.544151   \n",
       "66         0.517558        0.058905                6          0.546601   \n",
       "65         0.516129        0.025092                8          0.554870   \n",
       "45         0.512658        0.049466                9          0.534964   \n",
       "38         0.512250        0.013870               10          0.521640   \n",
       "24         0.512250        0.045925               10          0.536903   \n",
       "61         0.510004        0.036210               12          0.541650   \n",
       "78         0.508779        0.048860               13          0.533281   \n",
       "40         0.507758        0.023074               14          0.556402   \n",
       "67         0.504696        0.033944               15          0.540221   \n",
       "0          0.502246        0.075831               16          0.491220   \n",
       "97         0.501633        0.016316               17          0.507554   \n",
       "48         0.501021        0.048461               18          0.522101   \n",
       "90         0.500204        0.053573               19          0.541905   \n",
       "39         0.497346        0.051117               20          0.528229   \n",
       "50         0.495713        0.023894               21          0.510158   \n",
       "19         0.492650        0.019799               22          0.510004   \n",
       "30         0.492446        0.054264               23          0.508729   \n",
       "74         0.489792        0.042795               24          0.497961   \n",
       "31         0.485096        0.047097               25          0.514752   \n",
       "21         0.482646        0.027736               26          0.496019   \n",
       "94         0.480400        0.032080               27          0.482186   \n",
       "82         0.479992        0.056946               28          0.497194   \n",
       "77         0.479379        0.052097               29          0.493976   \n",
       "32         0.478767        0.033793               30          0.495102   \n",
       "..              ...             ...              ...               ...   \n",
       "57         0.448755        0.000276               69          0.448755   \n",
       "27         0.448755        0.000276               69          0.448755   \n",
       "6          0.448755        0.000276               69          0.448755   \n",
       "22         0.448755        0.000276               69          0.448755   \n",
       "25         0.447734        0.037776               75          0.449261   \n",
       "64         0.445284        0.006832               76          0.458709   \n",
       "96         0.444875        0.034879               77          0.449981   \n",
       "93         0.444671        0.017378               78          0.462282   \n",
       "8          0.442425        0.022256               79          0.473663   \n",
       "51         0.441200        0.034927               80          0.433596   \n",
       "69         0.440996        0.037203               81          0.469834   \n",
       "26         0.438955        0.022309               82          0.472744   \n",
       "28         0.435688        0.056668               83          0.461367   \n",
       "76         0.428134        0.029407               84          0.459881   \n",
       "1          0.426092        0.045511               85          0.426045   \n",
       "10         0.426092        0.045511               85          0.425943   \n",
       "73         0.425888        0.045609               87          0.425988   \n",
       "4          0.425888        0.045609               87          0.425988   \n",
       "2          0.425888        0.045609               87          0.425988   \n",
       "84         0.403430        0.055815               90          0.403233   \n",
       "55         0.403226        0.055812               91          0.403176   \n",
       "53         0.403226        0.055812               91          0.403278   \n",
       "71         0.402205        0.092963               93          0.402302   \n",
       "54         0.380359        0.055794               94          0.380409   \n",
       "91         0.379543        0.043471               95          0.388931   \n",
       "13         0.379339        0.092701               96          0.379536   \n",
       "56         0.355860        0.113829               97          0.355811   \n",
       "47         0.354635        0.078470               98          0.381637   \n",
       "29         0.333197        0.037567               99          0.365348   \n",
       "80         0.287464        0.057989              100          0.287465   \n",
       "\n",
       "    std_train_score  \n",
       "5          0.014917  \n",
       "89         0.011720  \n",
       "15         0.013078  \n",
       "86         0.012896  \n",
       "14         0.018428  \n",
       "75         0.007283  \n",
       "66         0.004793  \n",
       "65         0.015248  \n",
       "45         0.009001  \n",
       "38         0.021714  \n",
       "24         0.010180  \n",
       "61         0.011985  \n",
       "78         0.021914  \n",
       "40         0.010285  \n",
       "67         0.008791  \n",
       "0          0.025069  \n",
       "97         0.029193  \n",
       "48         0.009915  \n",
       "90         0.010981  \n",
       "39         0.024345  \n",
       "50         0.010530  \n",
       "19         0.012051  \n",
       "30         0.006890  \n",
       "74         0.027033  \n",
       "31         0.021719  \n",
       "21         0.026486  \n",
       "94         0.018857  \n",
       "82         0.014109  \n",
       "77         0.012971  \n",
       "32         0.018910  \n",
       "..              ...  \n",
       "57         0.000069  \n",
       "27         0.000069  \n",
       "6          0.000069  \n",
       "22         0.000069  \n",
       "25         0.037354  \n",
       "64         0.019880  \n",
       "96         0.038344  \n",
       "93         0.039597  \n",
       "8          0.011634  \n",
       "51         0.046929  \n",
       "69         0.009371  \n",
       "26         0.030580  \n",
       "28         0.076617  \n",
       "76         0.034246  \n",
       "1          0.045633  \n",
       "10         0.045582  \n",
       "73         0.045562  \n",
       "4          0.045562  \n",
       "2          0.045562  \n",
       "84         0.055892  \n",
       "55         0.055811  \n",
       "53         0.055894  \n",
       "71         0.092933  \n",
       "54         0.055815  \n",
       "91         0.037822  \n",
       "13         0.092721  \n",
       "56         0.113821  \n",
       "47         0.055107  \n",
       "29         0.042864  \n",
       "80         0.058017  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_res(random_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=200, n_jobs=1,\n",
       "          param_distributions={'activation': ['tanh', 'logistic'], 'alpha': [0.0001, 0.001, 0.01, 0.1, 0.05, 0.005, 0.0005], 'hidden_layer_sizes': [(50,), (100,), (200,)], 'learning_rate_init': [0.1, 0.5, 0.01, 0.005, 0.001, 0.0005, 0.0001]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distr = {'activation': ['tanh', 'logistic'],\n",
    "         'alpha': [1e-4, 1e-3, 1e-2, .1, .05,.005,.0005],\n",
    "         'hidden_layer_sizes': [(50,),(100,),(200,)],\n",
    "         'learning_rate_init': [.1,.5,.01,.005,.001,.0005,.0001]}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=distr, n_iter=200, cv=5, return_train_score=True)\n",
    "random_search.fit(features_raw, labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_learning_rate_init</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.541445</td>\n",
       "      <td>0.043868</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545019</td>\n",
       "      <td>0.007416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.539404</td>\n",
       "      <td>0.037376</td>\n",
       "      <td>2</td>\n",
       "      <td>0.545530</td>\n",
       "      <td>0.013965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.049322</td>\n",
       "      <td>3</td>\n",
       "      <td>0.556145</td>\n",
       "      <td>0.006807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.532666</td>\n",
       "      <td>0.027830</td>\n",
       "      <td>4</td>\n",
       "      <td>0.554001</td>\n",
       "      <td>0.011534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.531850</td>\n",
       "      <td>0.039208</td>\n",
       "      <td>5</td>\n",
       "      <td>0.536291</td>\n",
       "      <td>0.012550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.531441</td>\n",
       "      <td>0.040406</td>\n",
       "      <td>6</td>\n",
       "      <td>0.552573</td>\n",
       "      <td>0.009401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.530625</td>\n",
       "      <td>0.041972</td>\n",
       "      <td>7</td>\n",
       "      <td>0.551195</td>\n",
       "      <td>0.007415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.529808</td>\n",
       "      <td>0.033743</td>\n",
       "      <td>8</td>\n",
       "      <td>0.559412</td>\n",
       "      <td>0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.527766</td>\n",
       "      <td>0.043012</td>\n",
       "      <td>9</td>\n",
       "      <td>0.545937</td>\n",
       "      <td>0.009119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.527358</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>10</td>\n",
       "      <td>0.554718</td>\n",
       "      <td>0.019168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.525725</td>\n",
       "      <td>0.036291</td>\n",
       "      <td>11</td>\n",
       "      <td>0.543232</td>\n",
       "      <td>0.015942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.525521</td>\n",
       "      <td>0.046376</td>\n",
       "      <td>12</td>\n",
       "      <td>0.530729</td>\n",
       "      <td>0.018721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.049932</td>\n",
       "      <td>13</td>\n",
       "      <td>0.543639</td>\n",
       "      <td>0.008799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.522866</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>14</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.019126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.522458</td>\n",
       "      <td>0.018311</td>\n",
       "      <td>15</td>\n",
       "      <td>0.521284</td>\n",
       "      <td>0.004528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.521437</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>16</td>\n",
       "      <td>0.549254</td>\n",
       "      <td>0.013440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.521233</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>17</td>\n",
       "      <td>0.545479</td>\n",
       "      <td>0.011837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.519600</td>\n",
       "      <td>0.032902</td>\n",
       "      <td>18</td>\n",
       "      <td>0.544099</td>\n",
       "      <td>0.014554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.519600</td>\n",
       "      <td>0.031776</td>\n",
       "      <td>18</td>\n",
       "      <td>0.547060</td>\n",
       "      <td>0.009484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.519396</td>\n",
       "      <td>0.039447</td>\n",
       "      <td>20</td>\n",
       "      <td>0.538741</td>\n",
       "      <td>0.007331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.518987</td>\n",
       "      <td>0.036809</td>\n",
       "      <td>21</td>\n",
       "      <td>0.538844</td>\n",
       "      <td>0.011770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.517967</td>\n",
       "      <td>0.041585</td>\n",
       "      <td>22</td>\n",
       "      <td>0.548131</td>\n",
       "      <td>0.012633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.517967</td>\n",
       "      <td>0.025637</td>\n",
       "      <td>22</td>\n",
       "      <td>0.558086</td>\n",
       "      <td>0.017738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.516333</td>\n",
       "      <td>0.039301</td>\n",
       "      <td>24</td>\n",
       "      <td>0.539558</td>\n",
       "      <td>0.011366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.515108</td>\n",
       "      <td>0.035090</td>\n",
       "      <td>25</td>\n",
       "      <td>0.543845</td>\n",
       "      <td>0.009476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.514904</td>\n",
       "      <td>0.029179</td>\n",
       "      <td>26</td>\n",
       "      <td>0.548439</td>\n",
       "      <td>0.011745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>0.032002</td>\n",
       "      <td>27</td>\n",
       "      <td>0.559718</td>\n",
       "      <td>0.009466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.514292</td>\n",
       "      <td>0.033164</td>\n",
       "      <td>28</td>\n",
       "      <td>0.545224</td>\n",
       "      <td>0.018025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0001, 'hidden_layer_s...</td>\n",
       "      <td>0.514087</td>\n",
       "      <td>0.034850</td>\n",
       "      <td>29</td>\n",
       "      <td>0.537209</td>\n",
       "      <td>0.007076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.0005, 'hidden_layer_s...</td>\n",
       "      <td>0.513883</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>30</td>\n",
       "      <td>0.546551</td>\n",
       "      <td>0.017085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.477746</td>\n",
       "      <td>0.037963</td>\n",
       "      <td>71</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.032601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.01</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.01, 'hidden_layer_siz...</td>\n",
       "      <td>0.469988</td>\n",
       "      <td>0.022192</td>\n",
       "      <td>72</td>\n",
       "      <td>0.500459</td>\n",
       "      <td>0.013972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.005, 'hidden_layer_si...</td>\n",
       "      <td>0.456105</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>73</td>\n",
       "      <td>0.514193</td>\n",
       "      <td>0.028806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.449163</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>74</td>\n",
       "      <td>0.448908</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.426092</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>75</td>\n",
       "      <td>0.426045</td>\n",
       "      <td>0.045633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.426092</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>75</td>\n",
       "      <td>0.425943</td>\n",
       "      <td>0.045582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.425888</td>\n",
       "      <td>0.045609</td>\n",
       "      <td>77</td>\n",
       "      <td>0.425988</td>\n",
       "      <td>0.045562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.425684</td>\n",
       "      <td>0.045313</td>\n",
       "      <td>78</td>\n",
       "      <td>0.426096</td>\n",
       "      <td>0.045659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.403634</td>\n",
       "      <td>0.055317</td>\n",
       "      <td>79</td>\n",
       "      <td>0.403125</td>\n",
       "      <td>0.055873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.403430</td>\n",
       "      <td>0.056150</td>\n",
       "      <td>80</td>\n",
       "      <td>0.403272</td>\n",
       "      <td>0.055854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.403430</td>\n",
       "      <td>0.055979</td>\n",
       "      <td>80</td>\n",
       "      <td>0.403227</td>\n",
       "      <td>0.055853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.055812</td>\n",
       "      <td>82</td>\n",
       "      <td>0.403176</td>\n",
       "      <td>0.055811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.403022</td>\n",
       "      <td>0.055809</td>\n",
       "      <td>83</td>\n",
       "      <td>0.403221</td>\n",
       "      <td>0.055813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.403022</td>\n",
       "      <td>0.055809</td>\n",
       "      <td>83</td>\n",
       "      <td>0.403119</td>\n",
       "      <td>0.055938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.403022</td>\n",
       "      <td>0.055809</td>\n",
       "      <td>83</td>\n",
       "      <td>0.403221</td>\n",
       "      <td>0.055813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.402205</td>\n",
       "      <td>0.092963</td>\n",
       "      <td>86</td>\n",
       "      <td>0.402302</td>\n",
       "      <td>0.092933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.402205</td>\n",
       "      <td>0.092963</td>\n",
       "      <td>86</td>\n",
       "      <td>0.402302</td>\n",
       "      <td>0.092933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.080109</td>\n",
       "      <td>88</td>\n",
       "      <td>0.402671</td>\n",
       "      <td>0.056692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.05</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.381380</td>\n",
       "      <td>0.089469</td>\n",
       "      <td>89</td>\n",
       "      <td>0.380160</td>\n",
       "      <td>0.091810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.380972</td>\n",
       "      <td>0.090251</td>\n",
       "      <td>90</td>\n",
       "      <td>0.382553</td>\n",
       "      <td>0.087624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.380359</td>\n",
       "      <td>0.055794</td>\n",
       "      <td>91</td>\n",
       "      <td>0.380409</td>\n",
       "      <td>0.055815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.092836</td>\n",
       "      <td>92</td>\n",
       "      <td>0.379650</td>\n",
       "      <td>0.092834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.379543</td>\n",
       "      <td>0.054655</td>\n",
       "      <td>93</td>\n",
       "      <td>0.380823</td>\n",
       "      <td>0.056359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.356676</td>\n",
       "      <td>0.086709</td>\n",
       "      <td>94</td>\n",
       "      <td>0.356724</td>\n",
       "      <td>0.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.356676</td>\n",
       "      <td>0.086632</td>\n",
       "      <td>94</td>\n",
       "      <td>0.357036</td>\n",
       "      <td>0.086219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.005</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.333810</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>96</td>\n",
       "      <td>0.333957</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(200,)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.332993</td>\n",
       "      <td>0.103933</td>\n",
       "      <td>97</td>\n",
       "      <td>0.333038</td>\n",
       "      <td>0.103906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.311147</td>\n",
       "      <td>0.047282</td>\n",
       "      <td>98</td>\n",
       "      <td>0.311151</td>\n",
       "      <td>0.047385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.5</td>\n",
       "      <td>(50,)</td>\n",
       "      <td>0.01</td>\n",
       "      <td>logistic</td>\n",
       "      <td>{'learning_rate_init': 0.5, 'hidden_layer_size...</td>\n",
       "      <td>0.310331</td>\n",
       "      <td>0.087253</td>\n",
       "      <td>99</td>\n",
       "      <td>0.310232</td>\n",
       "      <td>0.087164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.1</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>{'learning_rate_init': 0.1, 'hidden_layer_size...</td>\n",
       "      <td>0.310127</td>\n",
       "      <td>0.087110</td>\n",
       "      <td>100</td>\n",
       "      <td>0.310373</td>\n",
       "      <td>0.087087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_learning_rate_init param_hidden_layer_sizes param_alpha  \\\n",
       "3                    0.0005                   (100,)        0.05   \n",
       "11                    0.001                    (50,)        0.01   \n",
       "47                    0.001                   (200,)       0.005   \n",
       "64                    0.001                   (200,)       0.001   \n",
       "41                     0.01                    (50,)        0.01   \n",
       "5                    0.0005                   (200,)      0.0005   \n",
       "77                    0.001                   (100,)      0.0005   \n",
       "76                    0.001                   (100,)       0.001   \n",
       "6                     0.001                   (100,)      0.0001   \n",
       "78                   0.0005                   (200,)        0.05   \n",
       "83                   0.0005                   (100,)      0.0001   \n",
       "42                     0.01                    (50,)         0.1   \n",
       "74                    0.005                   (200,)      0.0005   \n",
       "35                   0.0005                   (200,)      0.0005   \n",
       "91                     0.01                    (50,)       0.005   \n",
       "51                   0.0005                    (50,)         0.1   \n",
       "24                   0.0005                   (200,)         0.1   \n",
       "39                     0.01                    (50,)      0.0005   \n",
       "69                    0.001                   (100,)       0.005   \n",
       "8                    0.0001                   (200,)       0.005   \n",
       "70                    0.001                   (100,)         0.1   \n",
       "19                    0.001                    (50,)       0.005   \n",
       "87                   0.0005                   (100,)      0.0001   \n",
       "28                   0.0001                   (200,)       0.001   \n",
       "98                    0.005                    (50,)         0.1   \n",
       "4                     0.001                    (50,)      0.0001   \n",
       "73                   0.0005                   (100,)      0.0005   \n",
       "66                   0.0001                   (200,)         0.1   \n",
       "53                   0.0001                   (200,)      0.0005   \n",
       "63                   0.0005                    (50,)       0.005   \n",
       "..                      ...                      ...         ...   \n",
       "60                     0.01                   (200,)      0.0001   \n",
       "95                     0.01                   (200,)        0.01   \n",
       "22                    0.005                   (200,)       0.005   \n",
       "97                      0.1                   (100,)         0.1   \n",
       "65                      0.1                    (50,)      0.0001   \n",
       "68                      0.5                    (50,)       0.005   \n",
       "59                      0.1                   (200,)        0.01   \n",
       "33                      0.1                   (100,)        0.05   \n",
       "50                      0.1                    (50,)      0.0001   \n",
       "32                      0.1                    (50,)        0.05   \n",
       "79                      0.5                   (100,)      0.0005   \n",
       "92                      0.5                    (50,)       0.001   \n",
       "88                      0.5                   (100,)       0.005   \n",
       "48                      0.1                   (200,)       0.005   \n",
       "55                      0.5                    (50,)       0.001   \n",
       "99                      0.5                    (50,)         0.1   \n",
       "21                      0.1                   (100,)      0.0001   \n",
       "25                      0.5                   (200,)      0.0005   \n",
       "38                      0.1                   (100,)        0.05   \n",
       "67                      0.1                   (100,)      0.0005   \n",
       "57                      0.5                    (50,)      0.0001   \n",
       "9                       0.1                   (200,)       0.005   \n",
       "81                      0.1                   (100,)      0.0005   \n",
       "82                      0.5                   (200,)       0.005   \n",
       "7                       0.1                    (50,)         0.1   \n",
       "90                      0.1                   (100,)       0.005   \n",
       "12                      0.5                   (200,)      0.0005   \n",
       "56                      0.5                   (100,)       0.001   \n",
       "96                      0.5                    (50,)        0.01   \n",
       "89                      0.1                   (100,)       0.001   \n",
       "\n",
       "   param_activation                                             params  \\\n",
       "3          logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "11         logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "47         logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "64         logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "41             tanh  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "5          logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "77             tanh  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "76             tanh  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "6          logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "78             tanh  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "83         logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "42         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "74         logistic  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "35             tanh  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "91         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "51             tanh  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "24         logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "39         logistic  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "69             tanh  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "8          logistic  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "70         logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "19         logistic  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "87             tanh  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "28         logistic  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "98             tanh  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "4              tanh  {'learning_rate_init': 0.001, 'hidden_layer_si...   \n",
       "73             tanh  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "66             tanh  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "53         logistic  {'learning_rate_init': 0.0001, 'hidden_layer_s...   \n",
       "63         logistic  {'learning_rate_init': 0.0005, 'hidden_layer_s...   \n",
       "..              ...                                                ...   \n",
       "60             tanh  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "95             tanh  {'learning_rate_init': 0.01, 'hidden_layer_siz...   \n",
       "22             tanh  {'learning_rate_init': 0.005, 'hidden_layer_si...   \n",
       "97             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "65         logistic  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "68             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "59             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "33             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "50             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "32         logistic  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "79         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "92         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "88         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "48             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "55             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "99             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "21             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "25             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "38         logistic  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "67             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "57             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "9          logistic  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "81         logistic  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "82         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "7              tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "90             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "12         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "56             tanh  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "96         logistic  {'learning_rate_init': 0.5, 'hidden_layer_size...   \n",
       "89             tanh  {'learning_rate_init': 0.1, 'hidden_layer_size...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  mean_train_score  \\\n",
       "3          0.541445        0.043868                1          0.545019   \n",
       "11         0.539404        0.037376                2          0.545530   \n",
       "47         0.534300        0.049322                3          0.556145   \n",
       "64         0.532666        0.027830                4          0.554001   \n",
       "41         0.531850        0.039208                5          0.536291   \n",
       "5          0.531441        0.040406                6          0.552573   \n",
       "77         0.530625        0.041972                7          0.551195   \n",
       "76         0.529808        0.033743                8          0.559412   \n",
       "6          0.527766        0.043012                9          0.545937   \n",
       "78         0.527358        0.036596               10          0.554718   \n",
       "83         0.525725        0.036291               11          0.543232   \n",
       "42         0.525521        0.046376               12          0.530729   \n",
       "74         0.524500        0.049932               13          0.543639   \n",
       "35         0.522866        0.034483               14          0.556911   \n",
       "91         0.522458        0.018311               15          0.521284   \n",
       "51         0.521437        0.026748               16          0.549254   \n",
       "24         0.521233        0.024790               17          0.545479   \n",
       "39         0.519600        0.032902               18          0.544099   \n",
       "69         0.519600        0.031776               18          0.547060   \n",
       "8          0.519396        0.039447               20          0.538741   \n",
       "70         0.518987        0.036809               21          0.538844   \n",
       "19         0.517967        0.041585               22          0.548131   \n",
       "87         0.517967        0.025637               22          0.558086   \n",
       "28         0.516333        0.039301               24          0.539558   \n",
       "98         0.515108        0.035090               25          0.543845   \n",
       "4          0.514904        0.029179               26          0.548439   \n",
       "73         0.514700        0.032002               27          0.559718   \n",
       "66         0.514292        0.033164               28          0.545224   \n",
       "53         0.514087        0.034850               29          0.537209   \n",
       "63         0.513883        0.028317               30          0.546551   \n",
       "..              ...             ...              ...               ...   \n",
       "60         0.477746        0.037963               71          0.496991   \n",
       "95         0.469988        0.022192               72          0.500459   \n",
       "22         0.456105        0.051420               73          0.514193   \n",
       "97         0.449163        0.000225               74          0.448908   \n",
       "65         0.426092        0.045511               75          0.426045   \n",
       "68         0.426092        0.045511               75          0.425943   \n",
       "59         0.425888        0.045609               77          0.425988   \n",
       "33         0.425684        0.045313               78          0.426096   \n",
       "50         0.403634        0.055317               79          0.403125   \n",
       "32         0.403430        0.056150               80          0.403272   \n",
       "79         0.403430        0.055979               80          0.403227   \n",
       "92         0.403226        0.055812               82          0.403176   \n",
       "88         0.403022        0.055809               83          0.403221   \n",
       "48         0.403022        0.055809               83          0.403119   \n",
       "55         0.403022        0.055809               83          0.403221   \n",
       "99         0.402205        0.092963               86          0.402302   \n",
       "21         0.402205        0.092963               86          0.402302   \n",
       "25         0.387097        0.080109               88          0.402671   \n",
       "38         0.381380        0.089469               89          0.380160   \n",
       "67         0.380972        0.090251               90          0.382553   \n",
       "57         0.380359        0.055794               91          0.380409   \n",
       "9          0.379747        0.092836               92          0.379650   \n",
       "81         0.379543        0.054655               93          0.380823   \n",
       "82         0.356676        0.086709               94          0.356724   \n",
       "7          0.356676        0.086632               94          0.357036   \n",
       "90         0.333810        0.073400               96          0.333957   \n",
       "12         0.332993        0.103933               97          0.333038   \n",
       "56         0.311147        0.047282               98          0.311151   \n",
       "96         0.310331        0.087253               99          0.310232   \n",
       "89         0.310127        0.087110              100          0.310373   \n",
       "\n",
       "    std_train_score  \n",
       "3          0.007416  \n",
       "11         0.013965  \n",
       "47         0.006807  \n",
       "64         0.011534  \n",
       "41         0.012550  \n",
       "5          0.009401  \n",
       "77         0.007415  \n",
       "76         0.007394  \n",
       "6          0.009119  \n",
       "78         0.019168  \n",
       "83         0.015942  \n",
       "42         0.018721  \n",
       "74         0.008799  \n",
       "35         0.019126  \n",
       "91         0.004528  \n",
       "51         0.013440  \n",
       "24         0.011837  \n",
       "39         0.014554  \n",
       "69         0.009484  \n",
       "8          0.007331  \n",
       "70         0.011770  \n",
       "19         0.012633  \n",
       "87         0.017738  \n",
       "28         0.011366  \n",
       "98         0.009476  \n",
       "4          0.011745  \n",
       "73         0.009466  \n",
       "66         0.018025  \n",
       "53         0.007076  \n",
       "63         0.017085  \n",
       "..              ...  \n",
       "60         0.032601  \n",
       "95         0.013972  \n",
       "22         0.028806  \n",
       "97         0.000269  \n",
       "65         0.045633  \n",
       "68         0.045582  \n",
       "59         0.045562  \n",
       "33         0.045659  \n",
       "50         0.055873  \n",
       "32         0.055854  \n",
       "79         0.055853  \n",
       "92         0.055811  \n",
       "88         0.055813  \n",
       "48         0.055938  \n",
       "55         0.055813  \n",
       "99         0.092933  \n",
       "21         0.092933  \n",
       "25         0.056692  \n",
       "38         0.091810  \n",
       "67         0.087624  \n",
       "57         0.055815  \n",
       "9          0.092834  \n",
       "81         0.056359  \n",
       "82         0.086700  \n",
       "7          0.086219  \n",
       "90         0.073500  \n",
       "12         0.103906  \n",
       "56         0.047385  \n",
       "96         0.087164  \n",
       "89         0.087087  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_res(random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'CategoricalDtype' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-21b0d0632a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \"\"\"\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;31m# DataFrame), and store them. If not, store None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0mdtypes_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mdtypes_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'CategoricalDtype' has no len()"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "scores = cross_val_score(model, features_raw, labels_raw, cv=5, scoring=None)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'CategoricalDtype' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cda4858d72cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \"\"\"\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;31m# DataFrame), and store them. If not, store None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0mdtypes_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0mdtypes_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'CategoricalDtype' has no len()"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "distr = {'n_estimators': np.logspace(10,1000,200)}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=distr, n_iter=100, cv=5, return_train_score=True)\n",
    "random_search.fit(features_raw, labels_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-04223eb7266f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianProcessClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "model = GaussianProcessClassifier()\n",
    "scores = cross_val_score(model, features_raw, labels_raw, cv=5, scoring=None)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianProcessClassifier()\n",
    "\n",
    "distr = {}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=distr, n_iter=100, cv=5, return_train_score=True)\n",
    "random_search.fit(features_raw, labels_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5565597965437451"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RidgeClassifier(normalize=False, fit_intercept=True, alpha=.0017)\n",
    "scores = cross_val_score(model, features_raw, labels_raw, cv=5, scoring=None)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "        tol=0.001),\n",
       "          fit_params=None, iid=True, n_iter=1000, n_jobs=1,\n",
       "          param_distributions={'alpha': array([0.001  , 0.001  , ..., 0.00998, 0.01   ])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RidgeClassifier(normalize=False, fit_intercept=True)\n",
    "\n",
    "distr = {'alpha': np.logspace(-3,-2,1000)}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=distr, n_iter=1000, cv=5, return_train_score=True)\n",
    "random_search.fit(features_raw, labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.00166426</td>\n",
       "      <td>{'alpha': 0.0016642601764859037}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575694</td>\n",
       "      <td>0.008190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.00173476</td>\n",
       "      <td>{'alpha': 0.0017347593592339308}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.00173077</td>\n",
       "      <td>{'alpha': 0.0017307655341957258}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.00172678</td>\n",
       "      <td>{'alpha': 0.001726780903884356}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.00172281</td>\n",
       "      <td>{'alpha': 0.001722805447131394}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.00171884</td>\n",
       "      <td>{'alpha': 0.0017188391428171457}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.00171488</td>\n",
       "      <td>{'alpha': 0.001714881969870539}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.00171093</td>\n",
       "      <td>{'alpha': 0.0017109339072690151}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.00170699</td>\n",
       "      <td>{'alpha': 0.001706994934038408}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.00170307</td>\n",
       "      <td>{'alpha': 0.0017030650292528444}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.00169914</td>\n",
       "      <td>{'alpha': 0.001699144172034626}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.00169523</td>\n",
       "      <td>{'alpha': 0.0016952323415541197}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.00168744</td>\n",
       "      <td>{'alpha': 0.0016874356777273757}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.00169133</td>\n",
       "      <td>{'alpha': 0.0016913295170296488}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.00167967</td>\n",
       "      <td>{'alpha': 0.0016796748720926532}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.00167581</td>\n",
       "      <td>{'alpha': 0.0016758078645307671}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.00167195</td>\n",
       "      <td>{'alpha': 0.00167194975973199}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575694</td>\n",
       "      <td>0.008190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.0016681</td>\n",
       "      <td>{'alpha': 0.0016681005372000592}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575694</td>\n",
       "      <td>0.008190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.00166043</td>\n",
       "      <td>{'alpha': 0.0016604286571875295}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575796</td>\n",
       "      <td>0.008221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.00168355</td>\n",
       "      <td>{'alpha': 0.0016835508029612023}</td>\n",
       "      <td>0.556554</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.0017468</td>\n",
       "      <td>{'alpha': 0.0017467962151272458}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.00163009</td>\n",
       "      <td>{'alpha': 0.0016300923609797412}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575898</td>\n",
       "      <td>0.008257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.00165661</td>\n",
       "      <td>{'alpha': 0.0016566059589499134}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575796</td>\n",
       "      <td>0.008221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.00165279</td>\n",
       "      <td>{'alpha': 0.0016527920614648956}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575898</td>\n",
       "      <td>0.008257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.00164899</td>\n",
       "      <td>{'alpha': 0.001648986944471065}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.008261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.00164519</td>\n",
       "      <td>{'alpha': 0.0016451905877536625}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.008261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.0016414</td>\n",
       "      <td>{'alpha': 0.0016414029711444664}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041236</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.008261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.00173876</td>\n",
       "      <td>{'alpha': 0.0017387624002162504}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.00174277</td>\n",
       "      <td>{'alpha': 0.0017427746784089192}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.00175083</td>\n",
       "      <td>{'alpha': 0.001750827031735725}</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>21</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.008154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>0.00619144</td>\n",
       "      <td>{'alpha': 0.006191441755977848}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.006040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>0.0062344</td>\n",
       "      <td>{'alpha': 0.006234401888627864}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.006040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0.00622005</td>\n",
       "      <td>{'alpha': 0.006220048825634711}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.006040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>0.00807062</td>\n",
       "      <td>{'alpha': 0.008070620141149508}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>0.00805204</td>\n",
       "      <td>{'alpha': 0.008052039670825477}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>0.0080335</td>\n",
       "      <td>{'alpha': 0.008033501977124734}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>0.00801501</td>\n",
       "      <td>{'alpha': 0.008015006961565405}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>0.00799655</td>\n",
       "      <td>{'alpha': 0.007996554525892346}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>0.00797814</td>\n",
       "      <td>{'alpha': 0.007978144572076629}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.00795978</td>\n",
       "      <td>{'alpha': 0.007959777002314986}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>0.00794145</td>\n",
       "      <td>{'alpha': 0.00794145171902934}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.00792317</td>\n",
       "      <td>{'alpha': 0.007923168624866254}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.00790493</td>\n",
       "      <td>{'alpha': 0.00790492762269642}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574367</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.00788673</td>\n",
       "      <td>{'alpha': 0.007886728615614156}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574367</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.00786857</td>\n",
       "      <td>{'alpha': 0.007868571506936851}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574367</td>\n",
       "      <td>0.005965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0.00785046</td>\n",
       "      <td>{'alpha': 0.00785045620020451}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0.00783238</td>\n",
       "      <td>{'alpha': 0.007832382599179196}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0.00781435</td>\n",
       "      <td>{'alpha': 0.007814350607844541}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0.00779636</td>\n",
       "      <td>{'alpha': 0.0077963601304052365}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.00777841</td>\n",
       "      <td>{'alpha': 0.00777841107128649}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0.0077605</td>\n",
       "      <td>{'alpha': 0.007760503335133571}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.00774264</td>\n",
       "      <td>{'alpha': 0.007742636826811269}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0.00772481</td>\n",
       "      <td>{'alpha': 0.0077248114514034}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0.00812662</td>\n",
       "      <td>{'alpha': 0.008126619200091946}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574265</td>\n",
       "      <td>0.006042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.00694771</td>\n",
       "      <td>{'alpha': 0.00694771254846024}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574418</td>\n",
       "      <td>0.006173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>0.00626321</td>\n",
       "      <td>{'alpha': 0.006263207452198692}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574418</td>\n",
       "      <td>0.006033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>0.00624879</td>\n",
       "      <td>{'alpha': 0.006248788072006888}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.006040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>0.00810791</td>\n",
       "      <td>{'alpha': 0.008107909806731687}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041945</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574316</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>0.00620573</td>\n",
       "      <td>{'alpha': 0.0062057288067765}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574469</td>\n",
       "      <td>0.006040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.00693172</td>\n",
       "      <td>{'alpha': 0.006931717276155407}</td>\n",
       "      <td>0.554512</td>\n",
       "      <td>0.041616</td>\n",
       "      <td>948</td>\n",
       "      <td>0.574418</td>\n",
       "      <td>0.006173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_alpha                            params  mean_test_score  \\\n",
       "221  0.00166426  {'alpha': 0.0016642601764859037}         0.556554   \n",
       "239  0.00173476  {'alpha': 0.0017347593592339308}         0.556554   \n",
       "238  0.00173077  {'alpha': 0.0017307655341957258}         0.556554   \n",
       "237  0.00172678   {'alpha': 0.001726780903884356}         0.556554   \n",
       "236  0.00172281   {'alpha': 0.001722805447131394}         0.556554   \n",
       "235  0.00171884  {'alpha': 0.0017188391428171457}         0.556554   \n",
       "234  0.00171488   {'alpha': 0.001714881969870539}         0.556554   \n",
       "233  0.00171093  {'alpha': 0.0017109339072690151}         0.556554   \n",
       "232  0.00170699   {'alpha': 0.001706994934038408}         0.556554   \n",
       "231  0.00170307  {'alpha': 0.0017030650292528444}         0.556554   \n",
       "230  0.00169914   {'alpha': 0.001699144172034626}         0.556554   \n",
       "229  0.00169523  {'alpha': 0.0016952323415541197}         0.556554   \n",
       "227  0.00168744  {'alpha': 0.0016874356777273757}         0.556554   \n",
       "228  0.00169133  {'alpha': 0.0016913295170296488}         0.556554   \n",
       "225  0.00167967  {'alpha': 0.0016796748720926532}         0.556554   \n",
       "224  0.00167581  {'alpha': 0.0016758078645307671}         0.556554   \n",
       "223  0.00167195    {'alpha': 0.00167194975973199}         0.556554   \n",
       "222   0.0016681  {'alpha': 0.0016681005372000592}         0.556554   \n",
       "220  0.00166043  {'alpha': 0.0016604286571875295}         0.556554   \n",
       "226  0.00168355  {'alpha': 0.0016835508029612023}         0.556554   \n",
       "242   0.0017468  {'alpha': 0.0017467962151272458}         0.556350   \n",
       "212  0.00163009  {'alpha': 0.0016300923609797412}         0.556350   \n",
       "219  0.00165661  {'alpha': 0.0016566059589499134}         0.556350   \n",
       "218  0.00165279  {'alpha': 0.0016527920614648956}         0.556350   \n",
       "217  0.00164899   {'alpha': 0.001648986944471065}         0.556350   \n",
       "216  0.00164519  {'alpha': 0.0016451905877536625}         0.556350   \n",
       "215   0.0016414  {'alpha': 0.0016414029711444664}         0.556350   \n",
       "240  0.00173876  {'alpha': 0.0017387624002162504}         0.556350   \n",
       "241  0.00174277  {'alpha': 0.0017427746784089192}         0.556350   \n",
       "243  0.00175083   {'alpha': 0.001750827031735725}         0.556350   \n",
       "..          ...                               ...              ...   \n",
       "791  0.00619144   {'alpha': 0.006191441755977848}         0.554512   \n",
       "794   0.0062344   {'alpha': 0.006234401888627864}         0.554512   \n",
       "793  0.00622005   {'alpha': 0.006220048825634711}         0.554512   \n",
       "906  0.00807062   {'alpha': 0.008070620141149508}         0.554512   \n",
       "905  0.00805204   {'alpha': 0.008052039670825477}         0.554512   \n",
       "904   0.0080335   {'alpha': 0.008033501977124734}         0.554512   \n",
       "903  0.00801501   {'alpha': 0.008015006961565405}         0.554512   \n",
       "902  0.00799655   {'alpha': 0.007996554525892346}         0.554512   \n",
       "901  0.00797814   {'alpha': 0.007978144572076629}         0.554512   \n",
       "900  0.00795978   {'alpha': 0.007959777002314986}         0.554512   \n",
       "899  0.00794145    {'alpha': 0.00794145171902934}         0.554512   \n",
       "898  0.00792317   {'alpha': 0.007923168624866254}         0.554512   \n",
       "897  0.00790493    {'alpha': 0.00790492762269642}         0.554512   \n",
       "896  0.00788673   {'alpha': 0.007886728615614156}         0.554512   \n",
       "895  0.00786857   {'alpha': 0.007868571506936851}         0.554512   \n",
       "894  0.00785046    {'alpha': 0.00785045620020451}         0.554512   \n",
       "893  0.00783238   {'alpha': 0.007832382599179196}         0.554512   \n",
       "892  0.00781435   {'alpha': 0.007814350607844541}         0.554512   \n",
       "891  0.00779636  {'alpha': 0.0077963601304052365}         0.554512   \n",
       "890  0.00777841    {'alpha': 0.00777841107128649}         0.554512   \n",
       "889   0.0077605   {'alpha': 0.007760503335133571}         0.554512   \n",
       "888  0.00774264   {'alpha': 0.007742636826811269}         0.554512   \n",
       "887  0.00772481     {'alpha': 0.0077248114514034}         0.554512   \n",
       "909  0.00812662   {'alpha': 0.008126619200091946}         0.554512   \n",
       "841  0.00694771    {'alpha': 0.00694771254846024}         0.554512   \n",
       "796  0.00626321   {'alpha': 0.006263207452198692}         0.554512   \n",
       "795  0.00624879   {'alpha': 0.006248788072006888}         0.554512   \n",
       "908  0.00810791   {'alpha': 0.008107909806731687}         0.554512   \n",
       "792  0.00620573     {'alpha': 0.0062057288067765}         0.554512   \n",
       "840  0.00693172   {'alpha': 0.006931717276155407}         0.554512   \n",
       "\n",
       "     std_test_score  rank_test_score  mean_train_score  std_train_score  \n",
       "221        0.041315                1          0.575694         0.008190  \n",
       "239        0.041315                1          0.575541         0.008154  \n",
       "238        0.041315                1          0.575541         0.008154  \n",
       "237        0.041315                1          0.575541         0.008154  \n",
       "236        0.041315                1          0.575541         0.008154  \n",
       "235        0.041315                1          0.575541         0.008154  \n",
       "234        0.041315                1          0.575541         0.008154  \n",
       "233        0.041315                1          0.575541         0.008154  \n",
       "232        0.041315                1          0.575541         0.008154  \n",
       "231        0.041315                1          0.575541         0.008154  \n",
       "230        0.041315                1          0.575541         0.008154  \n",
       "229        0.041315                1          0.575541         0.008154  \n",
       "227        0.041315                1          0.575541         0.008154  \n",
       "228        0.041315                1          0.575541         0.008154  \n",
       "225        0.041315                1          0.575541         0.008154  \n",
       "224        0.041315                1          0.575541         0.008154  \n",
       "223        0.041315                1          0.575694         0.008190  \n",
       "222        0.041315                1          0.575694         0.008190  \n",
       "220        0.041315                1          0.575796         0.008221  \n",
       "226        0.041315                1          0.575541         0.008154  \n",
       "242        0.041661               21          0.575541         0.008154  \n",
       "212        0.041236               21          0.575898         0.008257  \n",
       "219        0.041236               21          0.575796         0.008221  \n",
       "218        0.041236               21          0.575898         0.008257  \n",
       "217        0.041236               21          0.575949         0.008261  \n",
       "216        0.041236               21          0.575949         0.008261  \n",
       "215        0.041236               21          0.575949         0.008261  \n",
       "240        0.041661               21          0.575541         0.008154  \n",
       "241        0.041661               21          0.575541         0.008154  \n",
       "243        0.041661               21          0.575541         0.008154  \n",
       "..              ...              ...               ...              ...  \n",
       "791        0.041616              948          0.574469         0.006040  \n",
       "794        0.041616              948          0.574469         0.006040  \n",
       "793        0.041616              948          0.574469         0.006040  \n",
       "906        0.041945              948          0.574316         0.005943  \n",
       "905        0.041945              948          0.574316         0.005943  \n",
       "904        0.041945              948          0.574316         0.005943  \n",
       "903        0.041945              948          0.574316         0.005943  \n",
       "902        0.041945              948          0.574316         0.005943  \n",
       "901        0.041945              948          0.574316         0.005943  \n",
       "900        0.041945              948          0.574316         0.005943  \n",
       "899        0.041945              948          0.574316         0.005943  \n",
       "898        0.041945              948          0.574316         0.005943  \n",
       "897        0.041945              948          0.574367         0.005965  \n",
       "896        0.041945              948          0.574367         0.005965  \n",
       "895        0.041945              948          0.574367         0.005965  \n",
       "894        0.041945              948          0.574316         0.005962  \n",
       "893        0.041945              948          0.574316         0.005962  \n",
       "892        0.041945              948          0.574316         0.005962  \n",
       "891        0.041945              948          0.574316         0.005962  \n",
       "890        0.041945              948          0.574316         0.005962  \n",
       "889        0.041945              948          0.574316         0.005962  \n",
       "888        0.041945              948          0.574316         0.005962  \n",
       "887        0.041945              948          0.574316         0.005962  \n",
       "909        0.041945              948          0.574265         0.006042  \n",
       "841        0.041616              948          0.574418         0.006173  \n",
       "796        0.041616              948          0.574418         0.006033  \n",
       "795        0.041616              948          0.574469         0.006040  \n",
       "908        0.041945              948          0.574316         0.005943  \n",
       "792        0.041616              948          0.574469         0.006040  \n",
       "840        0.041616              948          0.574418         0.006173  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_res(random_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "\n",
    "distr = {'kernel': ['linear', 'rbf', 'poly', 'sigmoid', 'precomputed']}\n",
    "\n",
    "random_search = GridSearchCV(model, param_grid=distr, cv=5, return_train_score=True)\n",
    "random_search.fit(features_raw, labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "scores = cross_val_score(model, features_raw, labels_raw, cv=5, scoring=None)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_raw.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
